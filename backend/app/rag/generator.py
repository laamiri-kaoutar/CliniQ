# from langchain.prompts import PromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from app.core.config import settings
from langchain_groq import ChatGroq
import mlflow

class MedicalGenerator:
    def __init__(self):
        # Dynamically pulling configuration from settings
        self.model_name = settings.GENERATOR_MODEL
        self.temperature = settings.GENERATOR_TEMP
        
        # self.llm = ChatGoogleGenerativeAI(
        #     model=self.model_name,
        #     # google_api_key=settings.GOOGLE_API_KEY,
        #     temperature=self.temperature  
        # )

        self.llm = ChatGroq(
            groq_api_key=settings.GROQ_API_KEY,
            model_name="llama-3.3-70b-versatile",
            temperature=0.2
        )
        
        template = """VOUS √äTES UN EXPERT EN AIDE √Ä LA D√âCISION CLINIQUE (CDSS).
        Votre mission est de transformer des extraits de protocoles en une r√©ponse synth√©tique, lisible et actionnable pour un m√©decin urgentiste.
        
        CADRE STRICT :
        1. BASE DE CONNAISSANCE : Utilisez UNIQUEMENT le contexte fourni. Ne faites appel √† aucune connaissance externe.
        2. ABSENCE D'INFO : Si le contexte ne contient pas la r√©ponse exacte, dites : "Les protocoles actuels ne contiennent pas d'information permettant de r√©pondre √† cette question."
        3. FORMATAGE UI : √âvitez les tableaux bruts. Utilisez des titres en gras, des listes √† puces a√©r√©es et des sauts de ligne clairs.
        
        R√àGLES D'URGENCE (PRIORIT√â ABSOLUE) :
        - Si le contexte mentionne "R√©f√©rer SAMU", "Urgence Vitale", ou "Avis sp√©cialis√© urgent", commencez la r√©ponse par la mention "üö® **URGENCE : R√âF√âRER SAMU IMM√âDIATEMENT**" en gras et en rouge (texte).
        
        STRUCTURE DE LA R√âPONSE :
        - **Alerte :** (Si applicable)
        - **Synth√®se Clinique :** Une explication fluide en 2-3 phrases.
        - **Actions Imm√©diates :** Liste √† puces des gestes √† faire.
        - **Points de Vigilance :** Signes de gravit√© √† surveiller.
        
        CONTEXTE M√âDICAL :
        {context}
        
        QUESTION DU PRATICIEN :
        {question}
        
        R√âPONSE CLINIQUE (SYNTH√àSE PROFESSIONNELLE) :"""

        self.prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        # Chain composition using LCEL
        self.chain = self.prompt | self.llm

    def generate(self, question, docs):
        # Synthesis of top-ranked clinical chunks
        context_text = "\n\n".join([doc.page_content for doc in docs])
        
        print(f"‚úçÔ∏è [Phase 4: G√©n√©ration] Synth√®se clinique via {self.model_name}...")
        response = self.chain.invoke({"context": context_text, "question": question})
        
        return response.content
    
    def log_params(self):
        """Logs the actual live parameters to MLflow"""
        mlflow.log_params({
            "generator_model": self.model_name,
            "generator_temperature": self.temperature,
            "template_version": "v1-clinical-strict"
        })